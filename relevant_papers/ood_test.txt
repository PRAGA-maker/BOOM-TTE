Note -- The paper finds that pretraining is extremely important. 

The base idea is on two first-principles / observations: 
1. ML models seem to be really inefficient at heterogenous populations. The mainstream regime is either train a highly specialized model using limited data (which fails on OOD -- this is why pretraining seems to be so important) or use a ton of data and spend a lot of resources generalizing it, but then on-inference your goal is to re-add that bias, which also takes a lot of resources. It seems really inefficient for models to spend so much resources on removing bias in data, then reapplying it on your one sample. This is the idea behind what I'm calling "Bayesian Bias Shifting", where instead you can shift bias with the synatax of do-calculus or something similar. This leads into: 
2. A good way to get better results in LLMs is to give examples. We follow this approach, where we identify what data we do have that is similar to what we want, and then identify how the differences between the two affect the KPI we care about. This is especially important in "highly hierarchical problems" (problems where we can observe only 1-ary or 2nd-ary effects or causal variables, but truely there are many more layers of variables inplay), because neural nets are bad at keeping track of all of these, but using this approach we can cut that space into a much smaller chunk, and because we're identifying truely casual effects we consume all of that deeper causal knowledge ie. all paths from causal variable A to B while treating the mechanism as unknown. This is the same reason we're able to use TTEs in medicine, because even without simulating the entire mechanism of the body, we are able to awnser questions based on observational data.


The goal is to I had an idea: the idea is just that instead of using RCTs to take individual data, generalizing witi loss bottleneck, then inferencing readding context priors etc however you frame it to refit the individual, instead we can simply create a method where we simply shift bias from person to person to refit them, no generalization required. so shifting bias â€”> more like do-calc so, in this case, let's take the HoF data. we could use, for now, RDKit Featurizer will work, or later I can train a model based on the PNAS paper. We take those variables representing our molecule (this long-term will be a big problem to work on, for now, this works), and use them as our causal variables here. We would say, ok what is our closest trainset data to our current molecule via something like a RDKIT vectorizer similarity search. Once we identify the closest, we featurize both. We then look at all the features. Say there are 100 features and 5 are different. For each different feature, we run a TTE to test how much that difference in features, say, molecular weight +5 affects our KPI we care about (in this case, HoF) iterating over all different features, would we be able to simply add up all of the differences? if we can assume independent variables then I think we can, but in this case we cannot, so not sure how to do this step. Do-calc or Bayesians will definiately have awnsers for us. The research plan for this would be to look at: one OOD and one ID molecule and see if we can achieve convergence: start from any 100 random molecules and arrive to the same prediction. This is an important causality check to see if we are seeing signal or how much IPTW is wrong. (or if we can washout IPTW bias by averaging a ton of predictions) if this is successful with successful convergence, then we go onto the 2nd test: benchmarking on OOD on BOOM really think about the core question: How to Combine Causal Effects? I feel like this is probably pretty fundamental / basic in causality theory that can be found in the literature.


This (above) idea is a bit uninfomred, putting it into good causal grammer, the solution would look like:

