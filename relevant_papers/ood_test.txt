Note -- The paper finds that pretraining is extremely important. 

The base idea is on two first-principles / observations: 
1. ML models seem to be really inefficient at heterogenous populations. The mainstream regime is either train a highly specialized model using limited data (which fails on OOD -- this is why pretraining seems to be so important) or use a ton of data and spend a lot of resources generalizing it, but then on-inference your goal is to re-add that bias, which also takes a lot of resources. It seems really inefficient for models to spend so much resources on removing bias in data, then reapplying it on your one sample. This is the idea behind what I'm calling "Bayesian Bias Shifting", where instead you can shift bias with the synatax of do-calculus or something similar. This leads into: 
2. A good way to get better results in LLMs is to give examples. We follow this approach, where we identify what data we do have that is similar to what we want, and then identify how the differences between the two affect the KPI we care about. This is especially important in "highly hierarchical problems" (problems where we can observe only 1-ary or 2nd-ary effects or causal variables, but truely there are many more layers of variables inplay), because neural nets are bad at keeping track of all of these, but using this approach we can cut that space into a much smaller chunk, and because we're identifying truely casual effects we consume all of that deeper causal knowledge ie. all paths from causal variable A to B while treating the mechanism as unknown. This is the same reason we're able to use TTEs in medicine, because even without simulating the entire mechanism of the body, we are able to awnser questions based on observational data.


The goal is to I had an idea: the idea is just that instead of using RCTs to take individual data, generalizing witi loss bottleneck, then inferencing readding context priors etc however you frame it to refit the individual, instead we can simply create a method where we simply shift bias from person to person to refit them, no generalization required. so shifting bias â€”> more like do-calc so, in this case, let's take the HoF data. we could use, for now, RDKit Featurizer will work, or later I can train a model based on the PNAS paper. We take those variables representing our molecule (this long-term will be a big problem to work on, for now, this works), and use them as our causal variables here. We would say, ok what is our closest trainset data to our current molecule via something like a RDKIT vectorizer similarity search. Once we identify the closest, we featurize both. We then look at all the features. Say there are 100 features and 5 are different. For each different feature, we run a TTE to test how much that difference in features, say, molecular weight +5 affects our KPI we care about (in this case, HoF) iterating over all different features, would we be able to simply add up all of the differences? if we can assume independent variables then I think we can, but in this case we cannot, so not sure how to do this step. Do-calc or Bayesians will definiately have awnsers for us. The research plan for this would be to look at: one OOD and one ID molecule and see if we can achieve convergence: start from any 100 random molecules and arrive to the same prediction. This is an important causality check to see if we are seeing signal or how much IPTW is wrong. (or if we can washout IPTW bias by averaging a ton of predictions) if this is successful with successful convergence, then we go onto the 2nd test: benchmarking on OOD on BOOM really think about the core question: How to Combine Causal Effects? I feel like this is probably pretty fundamental / basic in causality theory that can be found in the literature.


This (above) idea is a bit uninfomred, putting it into good causal grammer, the solution would look like:

Think of a modified treatment policy (MTP) as you hard-coding the â€œbias shiftâ€ you want the world to perform, then estimating the expected outcome under that policy-perturbed world using observed data. Formally, start with units 
ğ‘–
=
1
,
â€¦
,
ğ‘›
i=1,â€¦,n and observed tuples 
ğ‘‚
ğ‘–
=
(
ğ‘‹
ğ‘–
,
ğ´
ğ‘–
,
ğ‘Œ
ğ‘–
)
O
i
	â€‹

=(X
i
	â€‹

,A
i
	â€‹

,Y
i
	â€‹

), where 
ğ‘‹
X is pre-treatment context (your pretrained contextual embedding: scaffold family, size/heteroatom pattern, local environment encodings, etc.), 
ğ´
A is the exposure/treatment youâ€™re willing to intervene on, and 
ğ‘Œ
Y is the target (e.g., HoF). An MTP is a known mapping 
ğ‘‘
(
ğ‘
,
ğ‘¥
)
d(a,x) (deterministic policy) or a kernel 
ğ‘
(
ğ‘
âˆ£
ğ‘¥
)
q(aâˆ£x) (stochastic policy) that says how the exposure would be modified as a function of the natural value and context. The causal target is the population mean outcome under that policy, 
ğœ“
=
ğ¸
[
ğ‘Œ
ğ‘‘
(
ğ´
,
ğ‘‹
)
]
Ïˆ=E[Y
d(A,X)
] for deterministic 
ğ‘‘
d, or 
ğœ“
=
ğ¸
ğ‘‹
â€‰â£
âˆ‘
ğ‘
ğ‘š
(
ğ‘
,
ğ‘‹
)
â€‰
ğ‘
(
ğ‘
âˆ£
ğ‘‹
)
Ïˆ=E
X
	â€‹

âˆ‘
a
	â€‹

m(a,X)q(aâˆ£X) for stochastic 
ğ‘
q, with 
ğ‘š
(
ğ‘
,
ğ‘¥
)
=
ğ¸
[
ğ‘Œ
âˆ£
ğ´
=
ğ‘
,
ğ‘‹
=
ğ‘¥
]
m(a,x)=E[Yâˆ£A=a,X=x]. Identification under standard exchangeability/consistency requires no mechanistic model; it uses the g-formula 
ğœ“
=
ğ¸
{
ğ‘š
(
ğ‘‘
(
ğ´
,
ğ‘‹
)
,
ğ‘‹
)
}
Ïˆ=E{m(d(A,X),X)} (deterministic) or 
ğœ“
=
ğ¸
ğ‘‹
â€‰â£
âˆ‘
ğ‘
ğ‘š
(
ğ‘
,
ğ‘‹
)
ğ‘
(
ğ‘
âˆ£
ğ‘‹
)
Ïˆ=E
X
	â€‹

âˆ‘
a
	â€‹

m(a,X)q(aâˆ£X) (stochastic). This is exactly the â€œuse causal knowledge to make informed predictionsâ€ objective: youâ€™re not asking for the effect of an ill-posed â€œset Weight:=+5â€ intervention; youâ€™re asking for the outcome under a well-posed policy that shifts the exposure in a reproducible way. In continuous-exposure settings one very convenient class is additive shift MTPs 
ğ‘‘
ğ›¿
(
ğ‘
,
ğ‘¥
)
=
ğ‘
+
ğ›¿
d
Î´
	â€‹

(a,x)=a+Î´ (possibly truncated to the support), which deliver the â€œbias shiftâ€ you described but avoid extrapolating off the manifold because you only shift where feasible; in discrete settings (e.g., structural edits) you let 
ğ´
A be a finite set of edit types and define a policy 
ğ‘‘
d that maps â€œno-edit/edit-jâ€ to â€œapply minimal edit achieving â‰ˆ
ğ›¿
Î´-mass increase when feasible; otherwise leave as is.â€ These policies are the causal counterpart of your â€œBayesian bias shiftingâ€: they operate at the policy level (what shift to apply) rather than pretending a derived quantity (like weight) is an atomic cause. The estimation then becomes off-policy evaluation with doubly-robust learning: fit 
ğ‘š
(
ğ‘
,
ğ‘¥
)
m(a,x) and the observed treatment mechanism 
ğ‘”
(
ğ‘
âˆ£
ğ‘¥
)
=
ğ‘
(
ğ´
=
ğ‘
âˆ£
ğ‘‹
=
ğ‘¥
)
g(aâˆ£x)=p(A=aâˆ£X=x) using flexible ML (your pretrained contextual model is ideal for 
ğ‘š
m, and a conditional density/softmax model for 
ğ‘”
g), and combine them so you are consistent if either is right. For a continuous shift 
ğ´
â€™
=
ğ´
+
ğ›¿
Aâ€™=A+Î´ without truncation, the efficient one-step/AIPW estimator takes the simple, low-level form

ğœ“
^
DR
(
ğ›¿
)
=
1
ğ‘›
âˆ‘
ğ‘–
=
1
ğ‘›
[
ğ‘š
^
(
ğ´
ğ‘–
+
ğ›¿
,
ğ‘‹
ğ‘–
)
âŸ
g-compÂ term
+
(
ğ‘Œ
ğ‘–
âˆ’
ğ‘š
^
(
ğ´
ğ‘–
,
ğ‘‹
ğ‘–
)
)
ğ‘”
^
(
ğ´
ğ‘–
âˆ’
ğ›¿
âˆ£
ğ‘‹
ğ‘–
)
ğ‘”
^
(
ğ´
ğ‘–
âˆ£
ğ‘‹
ğ‘–
)
âŸ
augmentationÂ term
]
,
Ïˆ
^
	â€‹

DR
	â€‹

(Î´)=
n
1
	â€‹

i=1
âˆ‘
n
	â€‹

[
g-compÂ term
m
^
(A
i
	â€‹

+Î´,X
i
	â€‹

)
	â€‹

	â€‹

+
augmentationÂ term
(Y
i
	â€‹

âˆ’
m
^
(A
i
	â€‹

,X
i
	â€‹

))
g
^
	â€‹

(A
i
	â€‹

âˆ£X
i
	â€‹

)
g
^
	â€‹

(A
i
	â€‹

âˆ’Î´âˆ£X
i
	â€‹

)
	â€‹

	â€‹

	â€‹

],

with indicator/Jacobian factors added automatically when you clip the shift at boundaries (
ğ´
ğ‘–
Â±
ğ›¿
A
i
	â€‹

Â±Î´ must lie in the support). Intuition: the first term predicts what 
ğ‘Œ
Y would be if you nudged the exposure by 
ğ›¿
Î´ (your bias shift), evaluated with your learned outcome model; the second term debiases that prediction using importance ratios of the observed density to â€œpre-imageâ€ density under the shift, ensuring 
ğ‘›
n
	â€‹

-valid inference and robustness even when 
ğ‘š
^
m
^
 is imperfect. This exact formula (with the 
ğ‘”
^
(
ğ‘
âˆ’
ğ›¿
âˆ£
ğ‘¥
)
/
ğ‘”
^
(
ğ‘
âˆ£
ğ‘¥
)
g
^
	â€‹

(aâˆ’Î´âˆ£x)/
g
^
	â€‹

(aâˆ£x) ratio) is the efficient influence-functionâ€“based estimator for additive MTPs and is implemented in standard software; itâ€™s the workhorse that makes shift-policies practical with modern function learners. 
CRAN
+1

When the exposure is discrete (e.g., an edit 
ğ´
âˆˆ
{
0
,
1
,
â€¦
,
ğ½
}
Aâˆˆ{0,1,â€¦,J}), take a stochastic policy 
ğ‘
(
ğ‘
âˆ£
ğ‘¥
)
q(aâˆ£x) that embodies your version selection (e.g., â€œchoose the minimal +14â€‰Da edit with probability 1 if the local motif exists, else choose no-editâ€). The identified parameter becomes 
ğœ“
=
ğ¸
ğ‘‹
âˆ‘
ğ‘
ğ‘š
^
(
ğ‘
,
ğ‘‹
)
â€‰
ğ‘
(
ğ‘
âˆ£
ğ‘‹
)
Ïˆ=E
X
	â€‹

âˆ‘
a
	â€‹

m
^
(a,X)q(aâˆ£X). A doubly-robust estimator is then

ğœ“
^
DR
=
1
ğ‘›
âˆ‘
ğ‘–
=
1
ğ‘›
[
âˆ‘
ğ‘
ğ‘š
^
(
ğ‘
,
ğ‘‹
ğ‘–
)
â€‰
ğ‘
(
ğ‘
âˆ£
ğ‘‹
ğ‘–
)
â€…â€Š
+
â€…â€Š
ğ‘
(
ğ´
ğ‘–
âˆ£
ğ‘‹
ğ‘–
)
ğ‘”
^
(
ğ´
ğ‘–
âˆ£
ğ‘‹
ğ‘–
)
{
ğ‘Œ
ğ‘–
âˆ’
ğ‘š
^
(
ğ´
ğ‘–
,
ğ‘‹
ğ‘–
)
}
]
,
Ïˆ
^
	â€‹

DR
	â€‹

=
n
1
	â€‹

i=1
âˆ‘
n
	â€‹

[
a
âˆ‘
	â€‹

m
^
(a,X
i
	â€‹

)q(aâˆ£X
i
	â€‹

)+
g
^
	â€‹

(A
i
	â€‹

âˆ£X
i
	â€‹

)
q(A
i
	â€‹

âˆ£X
i
	â€‹

)
	â€‹

{Y
i
	â€‹

âˆ’
m
^
(A
i
	â€‹

,X
i
	â€‹

)}],

which you can read as plug-in under the target policy plus an IPW correction that reweights each observed outcome by how much the target policy would favor that action relative to how often it naturally occurred. This is the crisp, low-level way to â€œshift biasâ€ across heterogeneous version mixes without assuming all versions are equivalent; the policy is the versions. These stochastic-policy parameters were introduced precisely to handle feasible interventions and to relax positivity problems that plague deterministic â€œset-toâ€ interventions with continuous/multivalued 
ğ´
A. 
PMC
+1

Algorithmically for your HoF project you define 
ğ‘‹
X by pretraining a contextual encoder (your PNAS-style contextualized model) on diverse chemistry to produce sample-specific, pre-treatment embeddings that capture scaffold and local context; this is the variance-reduction and heterogeneity-modeling engine and corresponds to the â€œcontext encoderâ€ idea used for personalized modeling in heterogeneous populations. Then you pick your exposure and policy: if you insist on â€œweight,â€ use a continuous shift 
ğ‘‘
ğ›¿
(
ğ‘¤
,
ğ‘¥
)
=
clip
(
ğ‘¤
+
ğ›¿
)
d
Î´
	â€‹

(w,x)=clip(w+Î´) but restrict to contexts where both 
ğ‘¤
w and 
ğ‘¤
+
ğ›¿
w+Î´ are supported; if you prefer chemically faithful actions (recommended), let 
ğ´
A index a small library of MMP edits and define a deterministic/stochastic policy that maps the natural edit state to â€œapply the minimal edit that achieves â‰ˆ
ğ›¿
Î´ Da increase at the chosen site if feasible, else no-edit.â€ That policy prevents the amalgamation/hidden-versions problem because each base molecule/context has a single version dictated by 
ğ‘‘
d (deterministic) or a known version distribution 
ğ‘
q (stochastic). You then emulate a single, parallel-arm trial of the policy versus status-quo across all eligible molecules: (i) filter to eligibility 
ğ¼
{
policyÂ feasibleÂ atÂ 
(
ğ‘†
,
ğ‘‹
)
}
I{policyÂ feasibleÂ atÂ (S,X)}; (ii) fit 
ğ‘š
^
m
^
 with your best pretrained model (GNN/transformer + context) to map 
(
ğ´
,
ğ‘‹
)
â†’
ğ‘Œ
(A,X)â†’Y; (iii) fit 
ğ‘”
^
(
â‹…
âˆ£
ğ‘‹
)
g
^
	â€‹

(â‹…âˆ£X) (softmax for discrete 
ğ´
A; conditional density/flow for continuous 
ğ´
A); (iv) compute 
ğœ“
^
DR
Ïˆ
^
	â€‹

DR
	â€‹

 as above with cross-fitting; (v) get valid CIs from the empirical variance of the estimated efficient influence function. This â€œone big trialâ€ is exactly your bias-shifting objective, but it is policy-level and therefore well-posed and robust. The literature calls this class LMTP when there are multiple time points; at a single time point itâ€™s the same idea. These papers provide the identification, the 
ğ‘›
n
	â€‹

 theory, and efficient estimators you can drop in with your favorite ML for 
ğ‘š
^
,
ğ‘”
^
m
^
,
g
^
	â€‹

. 
EPI Research
+2
arXiv
+2

Why this fits your needs and why itâ€™s strong on OOD is that the estimand itself respects the data support and the chemical manifold. A naÃ¯ve â€œset 
ğ‘Š
:
=
ğ‘Š
+
ğ›¿
W:=W+Î´â€ demands outcomes for covariate cells that may not exist (classic positivity failure); the MTP only evaluates the shift where your mapping leaves you on-manifoldâ€”for a continuous shift you explicitly guard with clipping/eligibility; for discrete edits you only apply the edit in contexts observed in your corpus. That means the weighting ratio for the augmentation term (for continuous shifts itâ€™s 
ğ‘”
^
(
ğ´
âˆ’
ğ›¿
âˆ£
ğ‘‹
)
/
ğ‘”
^
(
ğ´
âˆ£
ğ‘‹
)
g
^
	â€‹

(Aâˆ’Î´âˆ£X)/
g
^
	â€‹

(Aâˆ£X)) is bounded in the regions you actually use, dramatically improving finite-sample stability. In other words, you are not extrapolating to â€œfantasy moleculesâ€; you are reweighting toward a realistic alternate world. Because BOOM defines OOD as property-tail extrapolation that defeats standard predictors, your MTP estimand lets you ask the right local question: â€œwhat is the expected HoF after the minimal feasible +14â€‰Da shift in this context?â€ rather than â€œwhat is the HoF for a hypothetical molecule with weight +14 and everything else unchanged,â€ and that aligns with discovery-regime evaluation while avoiding most of the tail-support pathologies. Empirically BOOM shows OOD generalization degrades badly and that pretraining choices matter; in this setup the pretraining helps nuisance learning (
ğ‘š
^
,
ğ‘”
^
m
^
,
g
^
	â€‹

) and context encoding, while the MTP prevents you from defining an impossible target in the first place. 

boom

 

boom

 

boom

You also asked about additivity and whether MSMs could â€œreweight wrong trials to respect additivity.â€ In this framework additivity is a modeling choice about the effect scale, not something IPTW can enforce. If you want the composition of multiple small shifts/edits to â€œadd,â€ choose a scale where itâ€™s plausible (often the log-scale for multiplicative composition), then either (a) evaluate small Î´-policies along a path (local linearity makes first-order additivity accurate; any departure shows up as second-order interaction terms you can estimate), or (b) fit a marginal structural model for 
ğ¸
[
ğ‘Œ
ğœ‹
]
E[Y
Ï€
] (or for the blip/contrast in a structural nested mean model) that includes only main effects plus heavily penalized interactions so that, unless the data demand it, interactions shrink toward zero and you recover near-additivity on your chosen scale. MSM/SNMM are complementary modeling shells for how you summarize the policy effect; the MTP defines the estimand and supplies the EIF/DR machinery. If forced to choose for your use-caseâ€”policy-level bias shifting with minimal extrapolation and OOD robustnessâ€”MTP beats a plain MSM (which, without a policy definition, collapses versions and revisits the SUTVA/amalgamation trap); use MSM/SNMM on top of MTP if you want a sparse, additive summary of multi-step edits. 
EPI Research
+1

Concretely, hereâ€™s how you would run it tomorrow for HoF. Build 
ğ‘‹
X with your contextual encoder (pretraining on large chemistry so 
ğ‘‹
X captures scaffold/local context heterogeneity, as advocated in contextualized modeling for heterogeneous populations). Define a tiny edit library and a deterministic Î´-policy 
ğ‘‘
d: â€œif the base molecule has a methyl-eligible site, replace H by CH
3
3
	â€‹

â€ (â‰ˆâ€‰+14â€‰Da); else leave it. Restrict to molecules where both treated/untreated versions are represented in your corpus (eligibility = positivity check). Fit 
ğ‘š
^
(
ğ‘
,
ğ‘¥
)
m
^
(a,x) (e.g., transformer/GNN + context) and 
ğ‘”
^
(
ğ‘
âˆ£
ğ‘¥
)
g
^
	â€‹

(aâˆ£x) (softmax over {no-edit, +CH
3
3
	â€‹

}). Compute 
ğœ“
^
DR
Ïˆ
^
	â€‹

DR
	â€‹

 for that one policy. If you want to walk from a known molecule to a target, define a sequence of such Î´-policies (each the â€œshortest MMP stepâ€ feasible in context), accumulate additive contributions on the log-scale (or another scale you pre-specify), and keep a tiny set of interaction terms with strong shrinkage; your estimator remains the same DR form, applied to each step/policy, with valid CIs from the EIF. This gives you your â€œvery simpleâ€ compositionâ€”with explicit uncertainty and a clean causal targetâ€”while keeping you on-manifold and letting the pretrained encoder do the heavy lifting in 
ğ‘š
m and 
ğ‘”
g. 

ellington-et-al-2025-learning-tâ€¦

 

ellington-et-al-2025-learning-tâ€¦

All of the above is standard, first-principles causal math for stochastic interventions/LMTPs (identification by the g-formula, efficient influence functions, and doubly-robust/targeted estimators), with off-the-shelf implementations for additive shifts (continuous 
ğ´
A) and policy-based interventions (discrete 
ğ´
A). Thatâ€™s why it maps so naturally to your â€œBayesian bias shiftingâ€ idea: you specify the bias shift as a policy, then do valid, variance-efficient off-policy evaluation with your contextual pretraining providing the inductive bias where it belongs (the nuisance learners), rather than asking a misspecified estimand to carry the load. 
PMC
+2
EPI Research
+2